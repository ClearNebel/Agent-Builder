import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, LogitsProcessor, LogitsProcessorList, TextIteratorStreamer
from threading import Thread
import os

class NanInfLogitsProcessor(LogitsProcessor):
    """
    A LogitsProcessor that detects and handles NaN or Inf values in the logits.
    This prevents crashes in torch.multinomial.
    """
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
        if torch.any(torch.isinf(scores)) or torch.any(torch.isnan(scores)):
            # You can either raise a more informative error or try to fix the scores
            print("!!! WARNING: Detected NaN or Inf in model logits. This indicates a model instability issue. !!!")
            # Forcing bad tokens to a very low value to prevent them from being sampled
            scores = torch.nan_to_num(scores, -1e9)
        return scores
    

def load_base_model_and_tokenizer(model_id: str):
    """
    Loads the base model and tokenizer ONCE.
    This is called at the start of the application.
    """
    print(f"[LLM Inference] Loading base model: '{model_id}'...")
    print("This may take a few minutes for the initial download...")

    attn_implementation = "sdpa"
    print(f"[LLM Inference] Using PyTorch's Scaled Dot Product Attention ({attn_implementation}). This is optimized for your hardware.")

    compute_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16
    print(f"[LLM Inference] Using compute dtype: {compute_dtype}")

    quantization_config = BitsAndBytesConfig(
        load_in_8bit=True,
        #load_in_4bit=True,
        #bnb_4bit_quant_type="nf4",
        #bnb_4bit_compute_dtype=compute_dtype,
        #bnb_4bit_use_double_quant=False,
    )

    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"[LLM Inference] Using device: {device}")

    tokenizer = AutoTokenizer.from_pretrained(model_id)
        
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        device_map=device,
        quantization_config=quantization_config,
        attn_implementation=attn_implementation,
        torch_dtype=compute_dtype
    )
    return model, tokenizer

def clean_chat_history(history: list) -> list:
    cleaned = []
    last_role = None

    for i, msg in enumerate(history):
        role = msg.get("role")
        content = msg.get("content", "").strip()

        if not content:
            continue

        if role == "system":
            if len(cleaned) == 0:
                cleaned.append({"role": "system", "content": content})
            continue 

        if len(cleaned) == 1 and role != "user":
            continue

        if last_role is None:
            last_role = cleaned[-1]["role"] if cleaned else None

        if role == last_role:
            continue

        cleaned.append({"role": role, "content": content})
        last_role = role

    return cleaned

def generate_response(
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    prompt_text: str,
    prompt_master_prompt: str = None,
    prompt_history: list = None,
    max_new_tokens: int = 4096,
    **kwargs
) -> str:
    """
    Generates a response from a pre-loaded LLM.
    """
    device = model.device

    temperature = kwargs.get('temperature', 0.7)
    top_p = kwargs.get('top_p', 0.9)
    
    chat = []
    
    if prompt_history:
        chat.extend(clean_chat_history(prompt_history))

    if prompt_master_prompt:
        chat.append({"role": "system", "content": prompt_master_prompt})
    

    chat.append({"role": "user", "content": prompt_text})

    prompt_for_model = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
    
    inputs = tokenizer(prompt_for_model, return_tensors="pt").to(device)

    logits_processor = LogitsProcessorList([NanInfLogitsProcessor()])

    outputs = model.generate(
        **inputs,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=temperature,
        top_p=top_p,
        logits_processor=logits_processor
    )

    response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)
    return response.strip()

def generate_response_stream(
    model: AutoModelForCausalLM,
    tokenizer: AutoTokenizer,
    prompt_text: str,
    max_new_tokens: int = 1024,
    **kwargs
) -> iter:
    """
    A generator function that yields tokens as they are generated by the model.
    """

    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    
    chat = [{"role": "user", "content": prompt_text}]
    prompt_for_model = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(prompt_for_model, return_tensors="pt").to(model.device)
    
    generation_kwargs = dict(
        inputs,
        streamer=streamer,
        max_new_tokens=max_new_tokens,
        do_sample=True,
        temperature=kwargs.get('temperature', 0.7),
        top_p=kwargs.get('top_p', 0.9)
    )
    
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()
    
    for new_text in streamer:
        yield new_text